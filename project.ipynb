{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "256fd2ae-49d7-4305-b2e1-089f2ec5cea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Data shape: (6000, 3)\n",
      "Windows: train (4161, 30, 3) val (900, 30, 3) test (900, 30, 3)\n",
      "Seq2SeqAttnModel(\n",
      "  (encoder): EncoderLSTM(\n",
      "    (lstm): LSTM(3, 64, batch_first=True, dropout=0.2)\n",
      "  )\n",
      "  (decoder): DecoderLSTMWithAttn(\n",
      "    (input_proj): Linear(in_features=67, out_features=64, bias=True)\n",
      "    (lstm): LSTM(64, 64, batch_first=True, dropout=0.2)\n",
      "    (attn): AdditiveAttention(\n",
      "      (W_enc): Linear(in_features=64, out_features=32, bias=False)\n",
      "      (W_dec): Linear(in_features=64, out_features=32, bias=False)\n",
      "      (v): Linear(in_features=32, out_features=1, bias=False)\n",
      "    )\n",
      "    (out): Linear(in_features=64, out_features=3, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saris\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (64x128 and 67x64)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 436\u001b[39m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 349\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, epochs + \u001b[32m1\u001b[39m):\n\u001b[32m    348\u001b[39m     train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch, device)\n\u001b[32m--> \u001b[39m\u001b[32m349\u001b[39m     val_loss, _, _, _ = \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    350\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m  Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m val_loss < best_val:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(model, dataloader, criterion, device)\u001b[39m\n\u001b[32m    238\u001b[39m Xb = Xb.to(device)\n\u001b[32m    239\u001b[39m Yb = Yb.to(device)\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m preds, attns = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# autoregressive inference\u001b[39;00m\n\u001b[32m    241\u001b[39m loss = criterion(preds, Yb)\n\u001b[32m    242\u001b[39m total_loss += loss.item() * Xb.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 209\u001b[39m, in \u001b[36mSeq2SeqAttnModel.forward\u001b[39m\u001b[34m(self, src, tgt, teacher_forcing_ratio)\u001b[39m\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# reshape dec_h/dec_c to (num_layers, B, dec_dim)\u001b[39;00m\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# h/c currently (num_layers, B, enc_dim) -> after linear same shape but dec_dim\u001b[39;00m\n\u001b[32m    208\u001b[39m dec_hidden = (dec_h, dec_c)\n\u001b[32m--> \u001b[39m\u001b[32m209\u001b[39m preds, attns = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_hidden\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_forcing_ratio\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m preds, attns\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 181\u001b[39m, in \u001b[36mDecoderLSTMWithAttn.forward\u001b[39m\u001b[34m(self, enc_outputs, dec_init_hidden, targets, teacher_forcing_ratio)\u001b[39m\n\u001b[32m    179\u001b[39m hidden = dec_init_hidden\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(out_len):\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m     y_pred, hidden, attn_w = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprev_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    182\u001b[39m     preds.append(y_pred.unsqueeze(\u001b[32m1\u001b[39m))\n\u001b[32m    183\u001b[39m     attns.append(attn_w.unsqueeze(\u001b[32m1\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 152\u001b[39m, in \u001b[36mDecoderLSTMWithAttn.forward_step\u001b[39m\u001b[34m(self, prev_y, enc_outputs, hidden)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;66;03m# combine prev_y and context\u001b[39;00m\n\u001b[32m    151\u001b[39m combined = torch.cat([prev_y, context], dim=\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, features+enc_dim)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m dec_input = torch.tanh(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m).unsqueeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B,1,dec_dim)\u001b[39;00m\n\u001b[32m    153\u001b[39m out, hidden = \u001b[38;5;28mself\u001b[39m.lstm(dec_input, hidden)\n\u001b[32m    154\u001b[39m out = out.squeeze(\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# (B, dec_dim)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (64x128 and 67x64)"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "ts_forecast_attention.py\n",
    "\n",
    "Self-contained multi-step time-series forecasting example using:\n",
    "- Synthetic multivariate data (>=5000 timesteps)\n",
    "- Preprocessing (min-max scaling)\n",
    "- Seq2Seq LSTM encoder-decoder with additive attention\n",
    "- Training loop, evaluation (RMSE, MAE, MASE)\n",
    "- Monte-Carlo dropout for prediction intervals\n",
    "- Attention visualization\n",
    "\n",
    "Dependencies:\n",
    "- numpy\n",
    "- torch\n",
    "- matplotlib\n",
    "\n",
    "Tested with: Python 3.9+, PyTorch 1.10+ (CPU or GPU)\n",
    "\"\"\"\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple\n",
    "\n",
    "# -----------------------------\n",
    "# Utilities: reproducibility\n",
    "# -----------------------------\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Generate synthetic dataset\n",
    "# -----------------------------\n",
    "def generate_multivariate_series(n_steps=6000, n_series=3, noise_std=0.2):\n",
    "    \"\"\"\n",
    "    Create synthetic multivariate time series mixing sinusoids, trends, and autoregression.\n",
    "    Returns array shape (n_steps, n_series)\n",
    "    \"\"\"\n",
    "    t = np.arange(n_steps)\n",
    "    data = []\n",
    "    for s in range(n_series):\n",
    "        freq = 0.01 + 0.01 * s\n",
    "        phase = s * 0.5\n",
    "        trend = 0.0005 * t * (1 + 0.2 * s)\n",
    "        seasonal = np.sin(2 * np.pi * freq * t + phase) + 0.5 * np.sin(2 * np.pi * freq * 3 * t + phase/2)\n",
    "        noise = np.random.normal(scale=noise_std, size=n_steps)\n",
    "        ar = np.zeros(n_steps)\n",
    "        for i in range(2, n_steps):\n",
    "            ar[i] = 0.6 * ar[i-1] - 0.2 * ar[i-2] + 0.05 * seasonal[i-1]\n",
    "        series = seasonal + trend + ar + noise\n",
    "        data.append(series)\n",
    "    data = np.stack(data, axis=1)  # shape (n_steps, n_series)\n",
    "    return data\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Preprocessing and windows\n",
    "# -----------------------------\n",
    "class MinMaxScaler:\n",
    "    def fit(self, data: np.ndarray):\n",
    "        self.min = data.min(axis=0)\n",
    "        self.max = data.max(axis=0)\n",
    "        # avoid division by zero\n",
    "        self.range = np.where(self.max - self.min == 0, 1.0, self.max - self.min)\n",
    "    def transform(self, data: np.ndarray):\n",
    "        return (data - self.min) / self.range\n",
    "    def inverse_transform(self, data: np.ndarray):\n",
    "        return data * self.range + self.min\n",
    "\n",
    "def create_windows(data: np.ndarray, in_len: int, out_len: int, step=1):\n",
    "    \"\"\"\n",
    "    data: (T, features)\n",
    "    returns X (N, in_len, features), Y (N, out_len, features)\n",
    "    \"\"\"\n",
    "    T, F = data.shape\n",
    "    Xs, Ys = [], []\n",
    "    for start in range(0, T - in_len - out_len + 1, step):\n",
    "        Xs.append(data[start : start + in_len])\n",
    "        Ys.append(data[start + in_len : start + in_len + out_len])\n",
    "    X = np.stack(Xs)  # (N, in_len, F)\n",
    "    Y = np.stack(Ys)  # (N, out_len, F)\n",
    "    return X, Y\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset class\n",
    "# -----------------------------\n",
    "class SeqDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = X.astype(np.float32)\n",
    "        self.Y = Y.astype(np.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Seq2Seq LSTM with Attention\n",
    "# -----------------------------\n",
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq, features)\n",
    "        out, (h, c) = self.lstm(x)\n",
    "        # out: (batch, seq, hidden)\n",
    "        return out, (h, c)\n",
    "\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim, attn_dim):\n",
    "        super().__init__()\n",
    "        self.W_enc = nn.Linear(enc_dim, attn_dim, bias=False)\n",
    "        self.W_dec = nn.Linear(dec_dim, attn_dim, bias=False)\n",
    "        self.v = nn.Linear(attn_dim, 1, bias=False)\n",
    "    def forward(self, enc_outputs, dec_hidden):\n",
    "        # enc_outputs: (batch, seq_enc, enc_dim)\n",
    "        # dec_hidden: (batch, dec_dim)  (we'll use hidden state from decoder)\n",
    "        # returns context (batch, enc_dim) and attn_weights (batch, seq_enc)\n",
    "        # compute scores\n",
    "        enc_proj = self.W_enc(enc_outputs)  # (B, S, A)\n",
    "        dec_proj = self.W_dec(dec_hidden).unsqueeze(1)  # (B, 1, A)\n",
    "        e = torch.tanh(enc_proj + dec_proj)  # (B, S, A)\n",
    "        scores = self.v(e).squeeze(-1)  # (B, S)\n",
    "        attn_weights = torch.softmax(scores, dim=1)  # (B, S)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs).squeeze(1)  # (B, enc_dim)\n",
    "        return context, attn_weights\n",
    "\n",
    "class DecoderLSTMWithAttn(nn.Module):\n",
    "    def __init__(self, input_dim, enc_dim, dec_dim, out_dim, attn_dim, num_layers=1, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim + enc_dim, dec_dim)  # combine previous target and context\n",
    "        self.lstm = nn.LSTM(dec_dim, dec_dim, num_layers=num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attn = AdditiveAttention(enc_dim, dec_dim, attn_dim)\n",
    "        self.out = nn.Linear(dec_dim, out_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward_step(self, prev_y, enc_outputs, hidden):\n",
    "        # prev_y: (B, features)  previous target / input to decoder\n",
    "        # enc_outputs: (B, seq, enc_dim)\n",
    "        # hidden: (h, c) where h: (num_layers, B, dec_dim)\n",
    "        dec_hidden = hidden[0][-1]  # last layer hidden (B, dec_dim)\n",
    "        context, attn_weights = self.attn(enc_outputs, dec_hidden)  # (B, enc_dim), (B, S)\n",
    "        # combine prev_y and context\n",
    "        combined = torch.cat([prev_y, context], dim=1)  # (B, features+enc_dim)\n",
    "        dec_input = torch.tanh(self.input_proj(combined)).unsqueeze(1)  # (B,1,dec_dim)\n",
    "        out, hidden = self.lstm(dec_input, hidden)\n",
    "        out = out.squeeze(1)  # (B, dec_dim)\n",
    "        out = self.dropout(out)\n",
    "        y_pred = self.out(out)  # (B, out_dim)\n",
    "        return y_pred, hidden, attn_weights\n",
    "\n",
    "    def forward(self, enc_outputs, dec_init_hidden, targets=None, teacher_forcing_ratio=0.5):\n",
    "        \"\"\"\n",
    "        enc_outputs: (B, seq_enc, enc_dim)\n",
    "        dec_init_hidden: tuple(h, c) where each is (num_layers, B, dec_dim)\n",
    "        targets: (B, out_len, out_dim) or None\n",
    "        returns preds (B, out_len, out_dim), attn_weights over time (B, out_len, seq_enc)\n",
    "        \"\"\"\n",
    "        B = enc_outputs.size(0)\n",
    "        device = enc_outputs.device\n",
    "        if targets is not None:\n",
    "            out_len = targets.size(1)\n",
    "            out_dim = targets.size(2)\n",
    "        else:\n",
    "            out_len = 10\n",
    "            out_dim = enc_outputs.size(2)  # fallback\n",
    "        preds = []\n",
    "        attns = []\n",
    "        # start token: use last input step from encoder as initial prev_y (or zeros)\n",
    "        # We'll let caller provide useful start; here use zeros\n",
    "        prev_y = torch.zeros(B, out_dim, device=device)\n",
    "        hidden = dec_init_hidden\n",
    "        for t in range(out_len):\n",
    "            y_pred, hidden, attn_w = self.forward_step(prev_y, enc_outputs, hidden)\n",
    "            preds.append(y_pred.unsqueeze(1))\n",
    "            attns.append(attn_w.unsqueeze(1))\n",
    "            # decide next prev_y\n",
    "            if targets is not None and random.random() < teacher_forcing_ratio:\n",
    "                prev_y = targets[:, t, :].to(device)\n",
    "            else:\n",
    "                prev_y = y_pred.detach()\n",
    "        preds = torch.cat(preds, dim=1)\n",
    "        attns = torch.cat(attns, dim=1)  # (B, out_len, seq_enc)\n",
    "        return preds, attns\n",
    "\n",
    "# Full model wrapper\n",
    "class Seq2SeqAttnModel(nn.Module):\n",
    "    def __init__(self, input_dim, enc_dim=64, dec_dim=64, attn_dim=32, out_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderLSTM(input_dim, enc_dim, num_layers=1, dropout=dropout)\n",
    "        self.decoder = DecoderLSTMWithAttn(out_dim if out_dim else input_dim, enc_dim, dec_dim, out_dim if out_dim else input_dim, attn_dim, num_layers=1, dropout=dropout)\n",
    "    def forward(self, src, tgt=None, teacher_forcing_ratio=0.5):\n",
    "        # src: (B, seq_in, features)\n",
    "        enc_out, (h, c) = self.encoder(src)\n",
    "        # initialize decoder hidden from encoder final states (project sizes if needed)\n",
    "        # if enc_dim != dec_dim, simple projection:\n",
    "        dec_h = torch.tanh(nn.Linear(enc_out.size(2), self.decoder.lstm.hidden_size).to(src.device)(h))\n",
    "        dec_c = torch.tanh(nn.Linear(enc_out.size(2), self.decoder.lstm.hidden_size).to(src.device)(c))\n",
    "        # reshape dec_h/dec_c to (num_layers, B, dec_dim)\n",
    "        # h/c currently (num_layers, B, enc_dim) -> after linear same shape but dec_dim\n",
    "        dec_hidden = (dec_h, dec_c)\n",
    "        preds, attns = self.decoder(enc_out, dec_hidden, targets=tgt, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        return preds, attns\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Training and evaluation\n",
    "# -----------------------------\n",
    "def train_epoch(model, dataloader, optimizer, criterion, epoch, device, clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for Xb, Yb in dataloader:\n",
    "        Xb = Xb.to(device)\n",
    "        Yb = Yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds, _ = model(Xb, tgt=Yb, teacher_forcing_ratio=0.6)\n",
    "        loss = criterion(preds, Yb)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * Xb.size(0)\n",
    "    return total_loss / len(dataloader.dataset)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_trues = []\n",
    "    all_attns = []\n",
    "    with torch.no_grad():\n",
    "        for Xb, Yb in dataloader:\n",
    "            Xb = Xb.to(device)\n",
    "            Yb = Yb.to(device)\n",
    "            preds, attns = model(Xb, tgt=None, teacher_forcing_ratio=0.0)  # autoregressive inference\n",
    "            loss = criterion(preds, Yb)\n",
    "            total_loss += loss.item() * Xb.size(0)\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "            all_trues.append(Yb.cpu().numpy())\n",
    "            all_attns.append(attns.cpu().numpy())\n",
    "    all_preds = np.concatenate(all_preds, axis=0)\n",
    "    all_trues = np.concatenate(all_trues, axis=0)\n",
    "    all_attns = np.concatenate(all_attns, axis=0)\n",
    "    return total_loss / len(dataloader.dataset), all_preds, all_trues, all_attns\n",
    "\n",
    "# Metrics\n",
    "def rmse(y_pred, y_true):\n",
    "    return np.sqrt(np.mean((y_pred - y_true) ** 2))\n",
    "def mae(y_pred, y_true):\n",
    "    return np.mean(np.abs(y_pred - y_true))\n",
    "def mase(y_pred, y_true, train_series):\n",
    "    # naive forecast: one-step naive (difference between consecutive points) averaged on training data\n",
    "    # implement seasonal naive with seasonality=1 (i.e., naive persistence)\n",
    "    # compute MAE of naive on training series\n",
    "    # train_series: (T, features)\n",
    "    naive_errors = np.abs(train_series[1:] - train_series[:-1])\n",
    "    mae_naive = np.mean(naive_errors)\n",
    "    mae_model = np.mean(np.abs(y_pred - y_true))\n",
    "    return mae_model / (mae_naive + 1e-8)\n",
    "\n",
    "# MC Dropout prediction intervals\n",
    "def mc_dropout_predict(model, X, n_samples=50):\n",
    "    \"\"\"\n",
    "    Keep dropout active and run forward n_samples times to get predictive distribution.\n",
    "    X: tensor (B, seq_in, features)\n",
    "    returns preds_samples: (n_samples, B, out_len, features)\n",
    "    \"\"\"\n",
    "    model.train()  # activate dropout\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_samples):\n",
    "            p, _ = model(X.to(device), tgt=None, teacher_forcing_ratio=0.0)\n",
    "            preds.append(p.cpu().numpy())\n",
    "    preds = np.stack(preds, axis=0)\n",
    "    model.eval()\n",
    "    return preds\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Main runner\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # hyperparams\n",
    "    n_steps = 6000\n",
    "    n_series = 3\n",
    "    in_len = 30\n",
    "    out_len = 10\n",
    "    batch_size = 64\n",
    "    epochs = 10  # small for demo; raise to 30-80 for better performance\n",
    "    lr = 1e-3\n",
    "    enc_dim = 64\n",
    "    dec_dim = 64\n",
    "    attn_dim = 32\n",
    "    dropout = 0.2\n",
    "\n",
    "    data = generate_multivariate_series(n_steps=n_steps, n_series=n_series, noise_std=0.25)\n",
    "    print(\"Data shape:\", data.shape)  # (T, features)\n",
    "\n",
    "    # split train/val/test (70/15/15)\n",
    "    T = data.shape[0]\n",
    "    train_end = int(T * 0.7)\n",
    "    val_end = int(T * 0.85)\n",
    "    train_data = data[:train_end]\n",
    "    val_data = data[train_end:val_end]\n",
    "    test_data = data[val_end:]\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(train_data)\n",
    "    data_scaled = scaler.transform(data)\n",
    "\n",
    "    # create windows over the full scaled data, but we'll split indices to keep time order\n",
    "    X_all, Y_all = create_windows(data_scaled, in_len=in_len, out_len=out_len, step=1)\n",
    "    # compute split indices corresponding to time splits - find first window index >= train_end - in_len - out_len + 1 mapping\n",
    "    # Simpler: map window start positions to time index\n",
    "    num_windows = X_all.shape[0]\n",
    "    # window start times\n",
    "    starts = np.arange(0, n_steps - in_len - out_len + 1)\n",
    "    train_mask = starts + in_len + out_len <= train_end\n",
    "    val_mask = (starts + in_len + out_len > train_end) & (starts + in_len + out_len <= val_end)\n",
    "    test_mask = starts + in_len + out_len > val_end\n",
    "\n",
    "    X_train, Y_train = X_all[train_mask], Y_all[train_mask]\n",
    "    X_val, Y_val = X_all[val_mask], Y_all[val_mask]\n",
    "    X_test, Y_test = X_all[test_mask], Y_all[test_mask]\n",
    "    print(\"Windows: train\", X_train.shape, \"val\", X_val.shape, \"test\", X_test.shape)\n",
    "\n",
    "    train_ds = SeqDataset(X_train, Y_train)\n",
    "    val_ds = SeqDataset(X_val, Y_val)\n",
    "    test_ds = SeqDataset(X_test, Y_test)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Seq2SeqAttnModel(input_dim=n_series, enc_dim=enc_dim, dec_dim=dec_dim, attn_dim=attn_dim, out_dim=n_series, dropout=dropout)\n",
    "    model.to(device)\n",
    "    print(model)\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val = float(\"inf\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, epoch, device)\n",
    "        val_loss, _, _, _ = evaluate(model, val_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch} Train Loss: {train_loss:.6f}  Val Loss: {val_loss:.6f}\")\n",
    "        if val_loss < best_val:\n",
    "            best_val = val_loss\n",
    "            torch.save(model.state_dict(), \"best_seq2seq_attn.pth\")\n",
    "\n",
    "    # load best\n",
    "    model.load_state_dict(torch.load(\"best_seq2seq_attn.pth\", map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_loss, preds, trues, attns = evaluate(model, test_loader, criterion, device)\n",
    "    print(\"Test MSE loss:\", test_loss)\n",
    "    # inverse scale preds and trues\n",
    "    preds_inv = scaler.inverse_transform(preds.reshape(-1, n_series)).reshape(preds.shape)\n",
    "    trues_inv = scaler.inverse_transform(trues.reshape(-1, n_series)).reshape(trues.shape)\n",
    "\n",
    "    # compute aggregate metrics\n",
    "    rmse_val = rmse(preds_inv, trues_inv)\n",
    "    mae_val = mae(preds_inv, trues_inv)\n",
    "    mase_val = mase(preds_inv, trues_inv, train_data)\n",
    "    print(f\"Test RMSE: {rmse_val:.6f}, MAE: {mae_val:.6f}, MASE: {mase_val:.6f}\")\n",
    "\n",
    "    # Plot sample predictions for first test batch entry\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sample_idx = 0\n",
    "    x0 = X_test[sample_idx]  # (in_len, features) scaled\n",
    "    y_true = Y_test[sample_idx]  # (out_len, features) scaled\n",
    "    # model prediction (single)\n",
    "    with torch.no_grad():\n",
    "        x_tensor = torch.tensor(x0).unsqueeze(0).to(device)\n",
    "        pred_scaled, attn = model(x_tensor, tgt=None, teacher_forcing_ratio=0.0)\n",
    "        pred_scaled = pred_scaled.cpu().numpy()[0]\n",
    "        attn = attn.cpu().numpy()[0]  # (out_len, seq_in)\n",
    "    x_all = np.concatenate([x0, y_true], axis=0)\n",
    "    x_all_inv = scaler.inverse_transform(x_all)\n",
    "    pred_inv = scaler.inverse_transform(pred_scaled)\n",
    "    # plot first series\n",
    "    for feat in range(n_series):\n",
    "        plt.subplot(n_series, 1, feat+1)\n",
    "        past = scaler.inverse_transform(x0)[:, feat]\n",
    "        true_future = scaler.inverse_transform(y_true)[:, feat]\n",
    "        pred_future = pred_inv[:, feat]\n",
    "        t_past = np.arange(-in_len, 0)\n",
    "        t_fut = np.arange(0, out_len)\n",
    "        plt.plot(t_past, past, label=\"input (past)\")\n",
    "        plt.plot(t_fut, true_future, 'o-', label=\"true future\")\n",
    "        plt.plot(t_fut, pred_future, 'x--', label=\"predicted future\")\n",
    "        plt.title(f\"Feature {feat}\")\n",
    "        if feat == 0:\n",
    "            plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot heatmap of attention weights for sample\n",
    "    plt.figure(figsize=(8, 3))\n",
    "    plt.imshow(attn.T, aspect='auto', origin='lower')\n",
    "    plt.xlabel(\"Decoder time step\")\n",
    "    plt.ylabel(\"Encoder time step\")\n",
    "    plt.title(\"Attention weights (encoder positions x decoder steps)\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # MC dropout intervals for that sample: run 100 MC forward passes\n",
    "    x_tensor = torch.tensor(x0).unsqueeze(0)\n",
    "    samples = mc_dropout_predict(model, x_tensor, n_samples=100)  # (n_samples, 1, out_len, features)\n",
    "    samples = samples[:, 0]  # remove batch dim\n",
    "    # compute 5th and 95th percentiles\n",
    "    lower = np.percentile(samples, 5, axis=0)\n",
    "    upper = np.percentile(samples, 95, axis=0)\n",
    "    median = np.median(samples, axis=0)\n",
    "    lower_inv = scaler.inverse_transform(lower)\n",
    "    upper_inv = scaler.inverse_transform(upper)\n",
    "    med_inv = scaler.inverse_transform(median)\n",
    "    # plot intervals for first feature\n",
    "    plt.figure(figsize=(8,4))\n",
    "    feat = 0\n",
    "    plt.plot(np.arange(out_len), scaler.inverse_transform(y_true)[:, feat], 'o-', label='true')\n",
    "    plt.plot(np.arange(out_len), med_inv[:, feat], 'x--', label='median pred')\n",
    "    plt.fill_between(np.arange(out_len), lower_inv[:, feat], upper_inv[:, feat], alpha=0.3, label='90% PI')\n",
    "    plt.legend()\n",
    "    plt.title(\"MC Dropout Prediction Intervals (sample)\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092438a3-b798-4d11-8b6d-42952373a9fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567f98a7-ecdf-42fd-b863-4421aa30cfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa6b01-d36c-458a-b99c-c8aa93424ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
